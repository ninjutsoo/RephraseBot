{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä RephraseBot Activity Report\n",
        "\n",
        "This notebook generates comprehensive activity reports for the RephraseBot (TweetRephraserBot).\n",
        "\n",
        "**Features:**\n",
        "- Date selection (today, yesterday, or specific date)\n",
        "- Time range filtering (EST timezone)\n",
        "- User activity statistics\n",
        "- Request analytics\n",
        "- Performance metrics\n",
        "- X/Twitter link tracking\n",
        "- Similarity score analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÖ Date Selection\n",
        "\n",
        "Enter the date you want to analyze. Supports:\n",
        "- `today` or `now` - Current date\n",
        "- `yesterday` - Previous day\n",
        "- Specific date formats: `YYYY-MM-DD`, `MM/DD/YYYY`, `DD-MM-YYYY`, `Jan 26, 2026`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÖ Target Date (EST): 2026-01-25 12:00 AM EST\n",
            "   UTC equivalent: 2026-01-25 05:00:00 UTC\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser\n",
        "import pytz\n",
        "from supabase import create_client, Client\n",
        "from typing import Optional, Tuple\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "if not SUPABASE_URL or not SUPABASE_KEY:\n",
        "    raise ValueError(\"Please set SUPABASE_URL and SUPABASE_KEY in .env file or environment variables\")\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Timezone setup\n",
        "UTC = pytz.UTC\n",
        "EST = pytz.timezone('US/Eastern')  # Handles DST automatically\n",
        "\n",
        "def parse_date_input(date_input: str) -> datetime:\n",
        "    \"\"\"Parse various date input formats\"\"\"\n",
        "    date_input = date_input.strip().lower()\n",
        "    \n",
        "    if date_input in ['today', 'now']:\n",
        "        return datetime.now(EST).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    elif date_input == 'yesterday':\n",
        "        return (datetime.now(EST) - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    else:\n",
        "        try:\n",
        "            # Try parsing with dateutil (handles many formats)\n",
        "            parsed = parser.parse(date_input)\n",
        "            # If no timezone, assume EST\n",
        "            if parsed.tzinfo is None:\n",
        "                parsed = EST.localize(parsed)\n",
        "            # Normalize to start of day in EST\n",
        "            return parsed.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Could not parse date: {date_input}. Error: {e}\")\n",
        "\n",
        "# DATE INPUT - Modify this value\n",
        "DATE_INPUT = \"yesterday\"  # Change to: \"today\", \"yesterday\", \"2026-01-26\", \"01/26/2026\", etc.\n",
        "\n",
        "target_date_est = parse_date_input(DATE_INPUT)\n",
        "print(f\"üìÖ Target Date (EST): {target_date_est.strftime('%Y-%m-%d %I:%M %p %Z')}\")\n",
        "print(f\"   UTC equivalent: {target_date_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è∞ Time Range Selection (EST)\n",
        "\n",
        "Enter the time range for analysis. Leave empty for full day.\n",
        "\n",
        "**Formats supported:**\n",
        "- `2pm to 4pm`\n",
        "- `14:00 to 16:00`\n",
        "- `2:00 PM to 4:00 PM`\n",
        "- `14:00-16:00`\n",
        "- Leave empty for full day (00:00 to 23:59)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∞ Time Range (EST): 12:00 AM to 11:59 PM\n",
            "   UTC equivalent: 2026-01-25 05:00:00 to 2026-01-26 04:59:59\n",
            "\n",
            "üìä Querying database for activity between:\n",
            "   EST: 2026-01-25 12:00 AM - 11:59 PM\n",
            "   UTC: 2026-01-25 05:00:00 - 04:59:59\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def parse_time_range(time_input: str, base_date: datetime) -> Tuple[datetime, datetime]:\n",
        "    \"\"\"Parse time range input and return start/end datetimes in EST\"\"\"\n",
        "    if not time_input or not time_input.strip():\n",
        "        # Full day\n",
        "        start = base_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        end = base_date.replace(hour=23, minute=59, second=59, microsecond=999999)\n",
        "        return start, end\n",
        "    \n",
        "    time_input = time_input.strip().lower()\n",
        "    \n",
        "    # Try to extract two times\n",
        "    # Patterns: \"2pm to 4pm\", \"14:00 to 16:00\", \"2:00 PM-4:00 PM\", etc.\n",
        "    patterns = [\n",
        "        r'(\\d{1,2})(?::(\\d{2}))?\\s*(am|pm)?\\s*(?:to|-)\\s*(\\d{1,2})(?::(\\d{2}))?\\s*(am|pm)?',\n",
        "        r'(\\d{1,2}):(\\d{2})\\s*(?:to|-)\\s*(\\d{1,2}):(\\d{2})',\n",
        "    ]\n",
        "    \n",
        "    start_time = None\n",
        "    end_time = None\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, time_input)\n",
        "        if match:\n",
        "            groups = match.groups()\n",
        "            \n",
        "            # Parse first time\n",
        "            if len(groups) >= 3:\n",
        "                hour1 = int(groups[0])\n",
        "                minute1 = int(groups[1]) if groups[1] else 0\n",
        "                ampm1 = groups[2].lower() if groups[2] else None\n",
        "                \n",
        "                if ampm1 == 'pm' and hour1 != 12:\n",
        "                    hour1 += 12\n",
        "                elif ampm1 == 'am' and hour1 == 12:\n",
        "                    hour1 = 0\n",
        "                \n",
        "                start_time = (hour1, minute1)\n",
        "                \n",
        "                # Parse second time\n",
        "                if len(groups) >= 6:\n",
        "                    hour2 = int(groups[3])\n",
        "                    minute2 = int(groups[4]) if groups[4] else 0\n",
        "                    ampm2 = groups[5].lower() if groups[5] else None\n",
        "                    \n",
        "                    if ampm2 == 'pm' and hour2 != 12:\n",
        "                        hour2 += 12\n",
        "                    elif ampm2 == 'am' and hour2 == 12:\n",
        "                        hour2 = 0\n",
        "                    \n",
        "                    end_time = (hour2, minute2)\n",
        "            \n",
        "            break\n",
        "    \n",
        "    if start_time is None or end_time is None:\n",
        "        raise ValueError(f\"Could not parse time range: {time_input}\")\n",
        "    \n",
        "    start_dt = base_date.replace(hour=start_time[0], minute=start_time[1], second=0, microsecond=0)\n",
        "    end_dt = base_date.replace(hour=end_time[0], minute=end_time[1], second=59, microsecond=999999)\n",
        "    \n",
        "    return start_dt, end_dt\n",
        "\n",
        "# TIME RANGE INPUT - Modify this value\n",
        "TIME_RANGE_INPUT = \"\"  # Empty for full day, or \"2pm to 4pm\", \"14:00 to 16:00\", etc.\n",
        "\n",
        "start_time_est, end_time_est = parse_time_range(TIME_RANGE_INPUT, target_date_est)\n",
        "\n",
        "print(f\"‚è∞ Time Range (EST): {start_time_est.strftime('%I:%M %p')} to {end_time_est.strftime('%I:%M %p')}\")\n",
        "print(f\"   UTC equivalent: {start_time_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S')} to {end_time_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Convert to UTC for database queries\n",
        "start_time_utc = start_time_est.astimezone(UTC)\n",
        "end_time_utc = end_time_est.astimezone(UTC)\n",
        "\n",
        "print(f\"\\nüìä Querying database for activity between:\")\n",
        "print(f\"   EST: {start_time_est.strftime('%Y-%m-%d %I:%M %p')} - {end_time_est.strftime('%I:%M %p')}\")\n",
        "print(f\"   UTC: {start_time_utc.strftime('%Y-%m-%d %H:%M:%S')} - {end_time_utc.strftime('%H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Database Table Check\n",
        "\n",
        "Before fetching data, let's verify the required tables exist in your Supabase database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking database tables...\n",
            "\n",
            "‚ùå Table 'activity_logs' NOT FOUND\n",
            "   üí° Hint: Found 'email_activity_logs' - you may need to rename or create 'activity_logs'\n",
            "‚ùå Table 'users' NOT FOUND\n",
            "   üí° Hint: Found 'email_users' - you may need to rename or create 'users'\n",
            "\n",
            "‚ö†Ô∏è  MISSING TABLES: activity_logs, users\n",
            "\n",
            "üìù To create these tables in Supabase, run the following SQL:\n",
            "\n",
            "======================================================================\n",
            "SQL for users table:\n",
            "======================================================================\n",
            "\n",
            "CREATE TABLE IF NOT EXISTS users (\n",
            "    user_id BIGINT PRIMARY KEY,\n",
            "    username TEXT,\n",
            "    first_name TEXT,\n",
            "    last_name TEXT,\n",
            "    language_code TEXT,\n",
            "    is_pro BOOLEAN DEFAULT FALSE,\n",
            "    pro_expires_at TIMESTAMPTZ,\n",
            "    trial_ends_at TIMESTAMPTZ,\n",
            "    preferred_tone TEXT,\n",
            "    preferred_length TEXT,\n",
            "    preferred_variation TEXT,\n",
            "    created_at TIMESTAMPTZ DEFAULT NOW(),\n",
            "    updated_at TIMESTAMPTZ DEFAULT NOW()\n",
            ");\n",
            "\n",
            "======================================================================\n",
            "SQL for activity_logs table:\n",
            "======================================================================\n",
            "\n",
            "CREATE TABLE IF NOT EXISTS activity_logs (\n",
            "    id BIGSERIAL PRIMARY KEY,\n",
            "    user_id BIGINT NOT NULL,\n",
            "    action_type TEXT NOT NULL,\n",
            "    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
            "    original_length INTEGER,\n",
            "    rephrased_length INTEGER,\n",
            "    similarity_score NUMERIC,\n",
            "    response_time_ms INTEGER,\n",
            "    had_x_link BOOLEAN,\n",
            "    was_forwarded BOOLEAN,\n",
            "    is_pro BOOLEAN,\n",
            "    error_type TEXT,\n",
            "    error_message TEXT,\n",
            "    created_at TIMESTAMPTZ DEFAULT NOW()\n",
            ");\n",
            "\n",
            "CREATE INDEX IF NOT EXISTS idx_tweet_activity_user_id ON activity_logs(user_id);\n",
            "CREATE INDEX IF NOT EXISTS idx_tweet_activity_timestamp ON activity_logs(timestamp);\n",
            "CREATE INDEX IF NOT EXISTS idx_tweet_activity_action_type ON activity_logs(action_type);\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üí° Alternatively, if you want to use existing 'email_*' tables,\n",
            "   update the table names in the next cell from 'tweet_*' to 'email_*'\n"
          ]
        }
      ],
      "source": [
        "# Check if required tables exist\n",
        "print(\"üîç Checking database tables...\\n\")\n",
        "\n",
        "required_tables = [\"activity_logs\", \"users\"]\n",
        "existing_tables = []\n",
        "missing_tables = []\n",
        "\n",
        "for table_name in required_tables:\n",
        "    try:\n",
        "        # Try a simple query to check if table exists\n",
        "        result = supabase.table(table_name).select(\"id\").limit(1).execute()\n",
        "        existing_tables.append(table_name)\n",
        "        print(f\"‚úÖ Table '{table_name}' exists\")\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"PGRST205\" in error_msg or \"not found\" in error_msg.lower():\n",
        "            missing_tables.append(table_name)\n",
        "            print(f\"‚ùå Table '{table_name}' NOT FOUND\")\n",
        "        else:\n",
        "            # Table exists but query failed for another reason\n",
        "            existing_tables.append(table_name)\n",
        "            print(f\"‚ö†Ô∏è  Table '{table_name}' exists but query failed: {error_msg[:100]}\")\n",
        "\n",
        "if missing_tables:\n",
        "    print(f\"\\n‚ö†Ô∏è  MISSING TABLES: {', '.join(missing_tables)}\")\n",
        "    print(\"\\nüìù Please ensure these tables exist in your Supabase database.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All required tables exist! Proceeding with data fetch...\")\n",
        "    print(\"   Note: activity_logs will be filtered by ='tweet' to show only RephraseBot data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Fetching Data from Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching activity logs...\n",
            "‚ö†Ô∏è ERROR: Table 'activity_logs' not found in database.\n",
            "   The table might not exist yet or might be named differently.\n",
            "   Please ensure the table exists in Supabase with the name 'activity_logs'\n",
            "   Or update the table name in this notebook if using a different name.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "# Fetch activity logs\n",
        "print(\"Fetching activity logs...\")\n",
        "# Note: activity_logs table (separate table for RephraseBot)\n",
        "try:\n",
        "    activity_logs = supabase.table(\"activity_logs\")\\\n",
        "        .select(\"*\")\\\n",
        "        .gte(\"timestamp\", start_time_utc.isoformat())\\\n",
        "        .lte(\"timestamp\", end_time_utc.isoformat())\\\n",
        "        .execute()\n",
        "    print(f\"‚úÖ Found {len(activity_logs.data)} activity log entries (='tweet')\")\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"activity_logs\" in error_msg and \"email_activity_logs\" in error_msg.lower():\n",
        "        print(\"‚ö†Ô∏è ERROR: Table 'activity_logs' not found in database.\")\n",
        "        print(\"   The table might not exist yet or might be named differently.\")\n",
        "        print(\"   Please ensure the table exists in Supabase with the name 'activity_logs'\")\n",
        "        print(\"   Or update the table name in this notebook if using a different name.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Fetch all users (for new user detection)\n",
        "print(\"\\nFetching user data...\")\n",
        "try:\n",
        "    all_users = supabase.table(\"users\")\\\n",
        "        .select(\"*\")\\\n",
        "        .execute()\n",
        "    print(f\"‚úÖ Found {len(all_users.data)} total users in database\")\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"users\" in error_msg:\n",
        "        print(\"‚ö†Ô∏è ERROR: Table 'users' not found in database.\")\n",
        "        print(\"   The table might not exist yet or might be named differently.\")\n",
        "        print(\"   Please ensure the table exists in Supabase with the name 'users'\")\n",
        "        print(\"   Or update the table name in this notebook if using a different name.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_logs = pd.DataFrame(activity_logs.data)\n",
        "df_users = pd.DataFrame(all_users.data)\n",
        "\n",
        "if len(df_logs) > 0:\n",
        "    # Convert timestamp to datetime and then to EST\n",
        "    # Parse timestamp - handle both timezone-aware and timezone-naive\n",
        "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])\n",
        "    \n",
        "    # Ensure timestamp is UTC-aware\n",
        "    if df_logs['timestamp'].dt.tz is None:\n",
        "        # Timezone-naive, localize to UTC\n",
        "        df_logs['timestamp_utc'] = df_logs['timestamp'].dt.tz_localize('UTC')\n",
        "    else:\n",
        "        # Already timezone-aware, convert to UTC\n",
        "        df_logs['timestamp_utc'] = df_logs['timestamp'].dt.tz_convert('UTC')\n",
        "    \n",
        "    # Convert to EST\n",
        "    df_logs['timestamp_est'] = df_logs['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
        "    df_logs['hour_est'] = df_logs['timestamp_est'].dt.hour\n",
        "    df_logs['date_est'] = df_logs['timestamp_est'].dt.date\n",
        "    \n",
        "    print(f\"\\nüìà Data Summary:\")\n",
        "    print(f\"   Total log entries: {len(df_logs)}\")\n",
        "    print(f\"   Unique users: {df_logs['user_id'].nunique()}\")\n",
        "    print(f\"   Date range: {df_logs['timestamp_est'].min()} to {df_logs['timestamp_est'].max()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No activity found for the specified time range\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• User Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data to analyze.\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    # Get all unique user IDs from the time period\n",
        "    active_user_ids = df_logs['user_id'].unique()\n",
        "    \n",
        "    # Find new users (first activity in this time period)\n",
        "    # Get first activity timestamp for each user in the period\n",
        "    first_activities = df_logs.groupby('user_id')['timestamp_utc'].min().reset_index()\n",
        "    first_activities.columns = ['user_id', 'first_activity_utc']\n",
        "    \n",
        "    # Check if this is their first activity ever (before this time period)\n",
        "    new_users = []\n",
        "    for user_id in active_user_ids:\n",
        "        user_first_activity = first_activities[first_activities['user_id'] == user_id]['first_activity_utc'].iloc[0]\n",
        "        \n",
        "        # Check if there's any activity before this time period\n",
        "        earlier_activity = supabase.table(\"activity_logs\")\\\n",
        "            .select(\"timestamp\")\\\n",
        "            .eq(\"user_id\", int(user_id))\\\n",
        "            .lt(\"timestamp\", start_time_utc.isoformat())\\\n",
        "            .limit(1)\\\n",
        "            .execute()\n",
        "        \n",
        "        if len(earlier_activity.data) == 0:\n",
        "            new_users.append(user_id)\n",
        "    \n",
        "    # Get user details\n",
        "    user_stats = df_logs.groupby('user_id').agg({\n",
        "        'action_type': 'count',\n",
        "        'timestamp_est': ['min', 'max'],\n",
        "        'response_time_ms': 'mean'\n",
        "    }).reset_index()\n",
        "    user_stats.columns = ['user_id', 'total_requests', 'first_request_est', 'last_request_est', 'avg_response_time_ms']\n",
        "    \n",
        "    # Merge with user info\n",
        "    if len(df_users) > 0:\n",
        "        user_stats = user_stats.merge(\n",
        "            df_users[['user_id', 'first_name', 'last_name', 'username', 'is_pro', 'pro_expires_at', 'trial_ends_at']],\n",
        "            on='user_id',\n",
        "            how='left'\n",
        "        )\n",
        "        user_stats['display_name'] = user_stats.apply(\n",
        "            lambda x: f\"{x['first_name'] or ''} {x['last_name'] or ''}\".strip() or f\"@{x['username']}\" if x['username'] else f\"User {x['user_id']}\",\n",
        "            axis=1\n",
        "        )\n",
        "    else:\n",
        "        user_stats['display_name'] = user_stats['user_id'].apply(lambda x: f\"User {x}\")\n",
        "    \n",
        "    user_stats['is_new_user'] = user_stats['user_id'].isin(new_users)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë• USER STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüìä Overview:\")\n",
        "    print(f\"   Total unique users: {len(active_user_ids)}\")\n",
        "    print(f\"   New users (first time): {len(new_users)}\")\n",
        "    print(f\"   Returning users: {len(active_user_ids) - len(new_users)}\")\n",
        "    \n",
        "    # New users list\n",
        "    if len(new_users) > 0:\n",
        "        print(f\"\\nüÜï NEW USERS ({len(new_users)}):\")\n",
        "        new_user_df = user_stats[user_stats['is_new_user']].sort_values('total_requests', ascending=False)\n",
        "        for idx, row in new_user_df.iterrows():\n",
        "            print(f\"   ‚Ä¢ {row['display_name']} (ID: {int(row['user_id'])}) - {int(row['total_requests'])} requests\")\n",
        "    else:\n",
        "        print(f\"\\nüÜï NEW USERS: None\")\n",
        "    \n",
        "    # All active users\n",
        "    print(f\"\\nüë§ ALL ACTIVE USERS ({len(user_stats)}):\")\n",
        "    user_stats_sorted = user_stats.sort_values('total_requests', ascending=False)\n",
        "    for idx, row in user_stats_sorted.iterrows():\n",
        "        status = \"üÜï NEW\" if row['is_new_user'] else \"üîÑ RETURNING\"\n",
        "        pro_status = \"üíé PRO\" if row.get('is_pro') else \"üÜì FREE\"\n",
        "        print(f\"   {status} {pro_status} {row['display_name']} (ID: {int(row['user_id'])}) - {int(row['total_requests'])} requests\")\n",
        "    \n",
        "    # Top users by requests\n",
        "    print(f\"\\nüèÜ TOP 10 USERS BY REQUESTS:\")\n",
        "    top_users = user_stats_sorted.head(10)\n",
        "    for idx, row in top_users.iterrows():\n",
        "        print(f\"   {int(row['total_requests']):3d} requests - {row['display_name']} (ID: {int(row['user_id'])})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Request Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data to analyze.\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä REQUEST STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Total requests\n",
        "    total_requests = len(df_logs)\n",
        "    print(f\"\\nüìà Total Requests: {total_requests}\")\n",
        "    \n",
        "    # Requests by action type\n",
        "    print(f\"\\nüìã Requests by Action Type:\")\n",
        "    action_counts = df_logs['action_type'].value_counts()\n",
        "    for action, count in action_counts.items():\n",
        "        percentage = (count / total_requests) * 100\n",
        "        print(f\"   {action:25s}: {count:4d} ({percentage:5.1f}%)\")\n",
        "    \n",
        "    # Success vs Error\n",
        "    success_count = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "    error_count = len(df_logs[df_logs['action_type'].isin([\n",
        "        'rephrase_failed', \n",
        "        'rate_limited', \n",
        "        'telegram_api_error', \n",
        "        'bot_blocked_by_user',\n",
        "        'forwarded_media_error',\n",
        "        'multiple_links_error',\n",
        "        'persian_error',\n",
        "        'invalid_channel'\n",
        "    ])])\n",
        "    other_count = total_requests - success_count - error_count\n",
        "    \n",
        "    print(f\"\\n‚úÖ Success Rate:\")\n",
        "    print(f\"   Successful rephrases: {success_count} ({(success_count/total_requests*100):.1f}%)\")\n",
        "    print(f\"   Errors: {error_count} ({(error_count/total_requests*100):.1f}%)\")\n",
        "    print(f\"   Other actions: {other_count} ({(other_count/total_requests*100):.1f}%)\")\n",
        "    \n",
        "    # Response time statistics\n",
        "    successful_logs = df_logs[df_logs['action_type'] == 'rephrase_success']\n",
        "    if len(successful_logs) > 0 and 'response_time_ms' in successful_logs.columns:\n",
        "        response_times = successful_logs['response_time_ms'].dropna()\n",
        "        if len(response_times) > 0:\n",
        "            print(f\"\\n‚è±Ô∏è  Response Time Statistics (for successful requests):\")\n",
        "            print(f\"   Average: {response_times.mean():.0f} ms ({response_times.mean()/1000:.2f} seconds)\")\n",
        "            print(f\"   Median:  {response_times.median():.0f} ms ({response_times.median()/1000:.2f} seconds)\")\n",
        "            print(f\"   Min:     {response_times.min():.0f} ms ({response_times.min()/1000:.2f} seconds)\")\n",
        "            print(f\"   Max:     {response_times.max():.0f} ms ({response_times.max()/1000:.2f} seconds)\")\n",
        "            print(f\"   P95:     {response_times.quantile(0.95):.0f} ms ({response_times.quantile(0.95)/1000:.2f} seconds)\")\n",
        "    \n",
        "    # Message/Tweet length statistics\n",
        "    if 'original_length' in df_logs.columns:\n",
        "        original_lengths = df_logs['original_length'].dropna()\n",
        "        if len(original_lengths) > 0:\n",
        "            print(f\"\\nüìè Message Length Statistics:\")\n",
        "            print(f\"   Average original length: {original_lengths.mean():.0f} characters\")\n",
        "            print(f\"   Median original length:  {original_lengths.median():.0f} characters\")\n",
        "            print(f\"   Min length: {original_lengths.min():.0f} characters\")\n",
        "            print(f\"   Max length: {original_lengths.max():.0f} characters\")\n",
        "    \n",
        "    if 'rephrased_length' in df_logs.columns:\n",
        "        rephrased_lengths = df_logs[df_logs['action_type'] == 'rephrase_success']['rephrased_length'].dropna()\n",
        "        if len(rephrased_lengths) > 0:\n",
        "            print(f\"   Average rephrased length: {rephrased_lengths.mean():.0f} characters\")\n",
        "            print(f\"   Median rephrased length:  {rephrased_lengths.median():.0f} characters\")\n",
        "            \n",
        "            # Length change\n",
        "            successful_with_both = df_logs[\n",
        "                (df_logs['action_type'] == 'rephrase_success') & \n",
        "                df_logs['original_length'].notna() & \n",
        "                df_logs['rephrased_length'].notna()\n",
        "            ]\n",
        "            if len(successful_with_both) > 0:\n",
        "                length_changes = successful_with_both['rephrased_length'] - successful_with_both['original_length']\n",
        "                print(f\"   Average length change: {length_changes.mean():+.0f} characters\")\n",
        "    \n",
        "    # Similarity score statistics\n",
        "    if 'similarity_score' in df_logs.columns:\n",
        "        similarity_scores = df_logs[df_logs['action_type'] == 'rephrase_success']['similarity_score'].dropna()\n",
        "        if len(similarity_scores) > 0:\n",
        "            print(f\"\\nüéØ Similarity Score Statistics:\")\n",
        "            print(f\"   Average similarity: {similarity_scores.mean():.3f}\")\n",
        "            print(f\"   Median similarity:  {similarity_scores.median():.3f}\")\n",
        "            print(f\"   Min similarity: {similarity_scores.min():.3f}\")\n",
        "            print(f\"   Max similarity: {similarity_scores.max():.3f}\")\n",
        "    \n",
        "    # X/Twitter link tracking\n",
        "    if 'had_x_link' in df_logs.columns:\n",
        "        x_link_count = df_logs[df_logs['action_type'] == 'rephrase_success']['had_x_link'].sum()\n",
        "        total_success = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "        if total_success > 0:\n",
        "            print(f\"\\nüîó X/Twitter Link Tracking:\")\n",
        "            print(f\"   Rephrases with X links: {int(x_link_count)} ({x_link_count/total_success*100:.1f}%)\")\n",
        "    \n",
        "    # Forwarded messages tracking\n",
        "    if 'was_forwarded' in df_logs.columns:\n",
        "        forwarded_count = df_logs[df_logs['action_type'] == 'rephrase_success']['was_forwarded'].sum()\n",
        "        total_success = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "        if total_success > 0:\n",
        "            print(f\"\\nüì§ Forwarded Messages:\")\n",
        "            print(f\"   Rephrases from forwarded messages: {int(forwarded_count)} ({forwarded_count/total_success*100:.1f}%)\")\n",
        "    \n",
        "    # Error breakdown\n",
        "    error_logs = df_logs[df_logs['action_type'].isin([\n",
        "        'rephrase_failed', \n",
        "        'rate_limited', \n",
        "        'telegram_api_error', \n",
        "        'bot_blocked_by_user',\n",
        "        'forwarded_media_error',\n",
        "        'multiple_links_error',\n",
        "        'persian_error',\n",
        "        'invalid_channel'\n",
        "    ])]\n",
        "    if len(error_logs) > 0:\n",
        "        print(f\"\\n‚ùå Error Breakdown:\")\n",
        "        error_types = error_logs['action_type'].value_counts()\n",
        "        for error_type, count in error_types.items():\n",
        "            print(f\"   {error_type:25s}: {count:4d}\")\n",
        "        \n",
        "        # Error messages (if available)\n",
        "        if 'error_message' in error_logs.columns:\n",
        "            error_messages = error_logs[error_logs['error_message'].notna()]['error_message'].value_counts()\n",
        "            if len(error_messages) > 0:\n",
        "                print(f\"\\n   Most common error messages:\")\n",
        "                for msg, count in error_messages.head(5).items():\n",
        "                    print(f\"      ‚Ä¢ {msg[:60]}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è∞ Activity Timeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data to analyze.\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚è∞ ACTIVITY TIMELINE (EST)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Activity by hour\n",
        "    print(f\"\\nüìä Activity by Hour (EST):\")\n",
        "    hourly_activity = df_logs.groupby('hour_est').size().reset_index(name='count')\n",
        "    hourly_activity = hourly_activity.sort_values('hour_est')\n",
        "    \n",
        "    max_count = hourly_activity['count'].max()\n",
        "    \n",
        "    for _, row in hourly_activity.iterrows():\n",
        "        hour = int(row['hour_est'])\n",
        "        count = int(row['count'])\n",
        "        hour_str = f\"{hour:2d}:00\"\n",
        "        bar_length = int((count / max_count) * 50) if max_count > 0 else 0\n",
        "        bar = \"‚ñà\" * bar_length\n",
        "        print(f\"   {hour_str} EST: {count:4d} requests {bar}\")\n",
        "    \n",
        "    # Peak hours\n",
        "    peak_hour = hourly_activity.loc[hourly_activity['count'].idxmax()]\n",
        "    print(f\"\\n   üèÜ Peak hour: {int(peak_hour['hour_est']):02d}:00 EST with {int(peak_hour['count'])} requests\")\n",
        "    \n",
        "    # Activity timeline (first and last activity)\n",
        "    print(f\"\\nüìÖ Activity Timeline:\")\n",
        "    print(f\"   First activity: {df_logs['timestamp_est'].min().strftime('%Y-%m-%d %I:%M:%S %p %Z')}\")\n",
        "    print(f\"   Last activity:  {df_logs['timestamp_est'].max().strftime('%Y-%m-%d %I:%M:%S %p %Z')}\")\n",
        "    \n",
        "    # Requests per minute (if time range is small)\n",
        "    time_span = (end_time_est - start_time_est).total_seconds() / 60  # minutes\n",
        "    if time_span <= 60:  # If less than 1 hour, show per-minute breakdown\n",
        "        print(f\"\\nüìä Activity by Minute (EST):\")\n",
        "        df_logs['minute_est'] = df_logs['timestamp_est'].dt.floor('T')\n",
        "        minute_activity = df_logs.groupby('minute_est').size().reset_index(name='count')\n",
        "        minute_activity = minute_activity.sort_values('minute_est')\n",
        "        \n",
        "        for _, row in minute_activity.iterrows():\n",
        "            minute = row['minute_est']\n",
        "            count = int(row['count'])\n",
        "            print(f\"   {minute.strftime('%H:%M')} EST: {count} requests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíé Pro vs Free Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data to analyze.\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üíé PRO vs FREE USERS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Merge with user data to get Pro status\n",
        "    if len(df_users) > 0:\n",
        "        df_logs_with_users = df_logs.merge(\n",
        "            df_users[['user_id', 'is_pro', 'pro_expires_at', 'trial_ends_at']],\n",
        "            on='user_id',\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Determine Pro status (check expiration dates)\n",
        "        now_utc = datetime.now(UTC)\n",
        "        \n",
        "        def is_pro_active(row):\n",
        "            if pd.isna(row.get('is_pro')) or not row.get('is_pro'):\n",
        "                # Check trial\n",
        "                if pd.notna(row.get('trial_ends_at')):\n",
        "                    trial_end = pd.to_datetime(row['trial_ends_at'])\n",
        "                    if trial_end.tz is None:\n",
        "                        trial_end = UTC.localize(trial_end)\n",
        "                    return trial_end > now_utc\n",
        "                return False\n",
        "            \n",
        "            # Check Pro expiration\n",
        "            if pd.isna(row.get('pro_expires_at')):\n",
        "                return True  # Lifetime Pro\n",
        "            \n",
        "            pro_end = pd.to_datetime(row['pro_expires_at'])\n",
        "            if pro_end.tz is None:\n",
        "                pro_end = UTC.localize(pro_end)\n",
        "            return pro_end > now_utc\n",
        "        \n",
        "        df_logs_with_users['is_pro_active'] = df_logs_with_users.apply(is_pro_active, axis=1)\n",
        "        \n",
        "        # Pro vs Free statistics\n",
        "        pro_users = df_logs_with_users[df_logs_with_users['is_pro_active'] == True]['user_id'].nunique()\n",
        "        free_users = df_logs_with_users[df_logs_with_users['is_pro_active'] == False]['user_id'].nunique()\n",
        "        \n",
        "        pro_requests = len(df_logs_with_users[df_logs_with_users['is_pro_active'] == True])\n",
        "        free_requests = len(df_logs_with_users[df_logs_with_users['is_pro_active'] == False])\n",
        "        \n",
        "        print(f\"\\nüë• User Count:\")\n",
        "        print(f\"   Pro users:  {pro_users}\")\n",
        "        print(f\"   Free users: {free_users}\")\n",
        "        \n",
        "        print(f\"\\nüìä Request Count:\")\n",
        "        print(f\"   Pro requests:  {pro_requests} ({(pro_requests/len(df_logs)*100):.1f}%)\")\n",
        "        print(f\"   Free requests: {free_requests} ({(free_requests/len(df_logs)*100):.1f}%)\")\n",
        "        \n",
        "        if pro_users > 0:\n",
        "            avg_pro_requests = pro_requests / pro_users\n",
        "            print(f\"   Average requests per Pro user: {avg_pro_requests:.1f}\")\n",
        "        \n",
        "        if free_users > 0:\n",
        "            avg_free_requests = free_requests / free_users\n",
        "            print(f\"   Average requests per Free user: {avg_free_requests:.1f}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è User data not available for Pro/Free analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data to analyze.\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìà SUMMARY REPORT\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüìÖ Date: {target_date_est.strftime('%Y-%m-%d (%A)')}\")\n",
        "    print(f\"‚è∞ Time Range: {start_time_est.strftime('%I:%M %p')} - {end_time_est.strftime('%I:%M %p')} EST\")\n",
        "    print(f\"\\nüìä Key Metrics:\")\n",
        "    print(f\"   ‚Ä¢ Total Requests: {len(df_logs)}\")\n",
        "    print(f\"   ‚Ä¢ Unique Users: {df_logs['user_id'].nunique()}\")\n",
        "    \n",
        "    success_count = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "    print(f\"   ‚Ä¢ Successful Rephrases: {success_count} ({(success_count/len(df_logs)*100):.1f}%)\")\n",
        "    \n",
        "    if len(new_users) > 0:\n",
        "        print(f\"   ‚Ä¢ New Users: {len(new_users)}\")\n",
        "    \n",
        "    if len(df_logs) > 0:\n",
        "        response_times = df_logs[df_logs['action_type'] == 'rephrase_success']['response_time_ms'].dropna()\n",
        "        if len(response_times) > 0:\n",
        "            print(f\"   ‚Ä¢ Avg Response Time: {response_times.mean()/1000:.2f} seconds\")\n",
        "    \n",
        "    peak_hour_data = df_logs.groupby('hour_est').size()\n",
        "    if len(peak_hour_data) > 0:\n",
        "        peak_hour = peak_hour_data.idxmax()\n",
        "        peak_count = peak_hour_data.max()\n",
        "        print(f\"   ‚Ä¢ Peak Hour: {int(peak_hour):02d}:00 EST ({peak_count} requests)\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
