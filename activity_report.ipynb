{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä RephraseBot Activity Report\n",
        "\n",
        "This notebook generates comprehensive activity reports for the RephraseBot (TweetRephraserBot).\n",
        "\n",
        "**Features:**\n",
        "- Date selection (today, yesterday, or specific date)\n",
        "- Time range filtering (EST timezone)\n",
        "- User activity statistics\n",
        "- Request analytics\n",
        "- Performance metrics\n",
        "- X/Twitter link tracking\n",
        "- Similarity score analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÖ Date Selection\n",
        "\n",
        "Enter the date you want to analyze. Supports:\n",
        "- `today` or `now` - Current date\n",
        "- `yesterday` - Previous day\n",
        "- Specific date formats: `YYYY-MM-DD`, `MM/DD/YYYY`, `DD-MM-YYYY`, `Jan 26, 2026`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÖ Target Date (EST): 2026-01-25 12:00 AM EST\n",
            "   UTC equivalent: 2026-01-25 05:00:00 UTC\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser\n",
        "import pytz\n",
        "from supabase import create_client, Client\n",
        "from typing import Optional, Tuple\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "if not SUPABASE_URL or not SUPABASE_KEY:\n",
        "    raise ValueError(\"Please set SUPABASE_URL and SUPABASE_KEY in .env file or environment variables\")\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Timezone setup\n",
        "UTC = pytz.UTC\n",
        "EST = pytz.timezone('US/Eastern')  # Handles DST automatically\n",
        "\n",
        "def parse_date_input(date_input: str) -> datetime:\n",
        "    \"\"\"Parse various date input formats\"\"\"\n",
        "    date_input = date_input.strip().lower()\n",
        "    \n",
        "    if date_input in ['today', 'now']:\n",
        "        return datetime.now(EST).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    elif date_input == 'yesterday':\n",
        "        return (datetime.now(EST) - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    else:\n",
        "        try:\n",
        "            # Try parsing with dateutil (handles many formats)\n",
        "            parsed = parser.parse(date_input)\n",
        "            # If no timezone, assume EST\n",
        "            if parsed.tzinfo is None:\n",
        "                parsed = EST.localize(parsed)\n",
        "            # Normalize to start of day in EST\n",
        "            return parsed.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Could not parse date: {date_input}. Error: {e}\")\n",
        "\n",
        "# DATE INPUT - Modify this value\n",
        "DATE_INPUT = \"yesterday\"  # Change to: \"today\", \"yesterday\", \"2026-01-26\", \"01/26/2026\", etc.\n",
        "\n",
        "target_date_est = parse_date_input(DATE_INPUT)\n",
        "print(f\"üìÖ Target Date (EST): {target_date_est.strftime('%Y-%m-%d %I:%M %p %Z')}\")\n",
        "print(f\"   UTC equivalent: {target_date_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è∞ Time Range Selection (EST)\n",
        "\n",
        "Enter the time range for analysis. Leave empty for full day.\n",
        "\n",
        "**Formats supported:**\n",
        "- `2pm to 4pm`\n",
        "- `14:00 to 16:00`\n",
        "- `2:00 PM to 4:00 PM`\n",
        "- `14:00-16:00`\n",
        "- Leave empty for full day (00:00 to 23:59)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∞ Time Range (EST): 12:00 AM to 11:59 PM\n",
            "   UTC equivalent: 2026-01-25 05:00:00 to 2026-01-26 04:59:59\n",
            "\n",
            "üìä Querying database for activity between:\n",
            "   EST: 2026-01-25 12:00 AM - 11:59 PM\n",
            "   UTC: 2026-01-25 05:00:00 - 04:59:59\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def parse_time_range(time_input: str, base_date: datetime) -> Tuple[datetime, datetime]:\n",
        "    \"\"\"Parse time range input and return start/end datetimes in EST\"\"\"\n",
        "    if not time_input or not time_input.strip():\n",
        "        # Full day\n",
        "        start = base_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        end = base_date.replace(hour=23, minute=59, second=59, microsecond=999999)\n",
        "        return start, end\n",
        "    \n",
        "    time_input = time_input.strip().lower()\n",
        "    \n",
        "    # Try to extract two times\n",
        "    # Patterns: \"2pm to 4pm\", \"14:00 to 16:00\", \"2:00 PM-4:00 PM\", etc.\n",
        "    patterns = [\n",
        "        r'(\\d{1,2})(?::(\\d{2}))?\\s*(am|pm)?\\s*(?:to|-)\\s*(\\d{1,2})(?::(\\d{2}))?\\s*(am|pm)?',\n",
        "        r'(\\d{1,2}):(\\d{2})\\s*(?:to|-)\\s*(\\d{1,2}):(\\d{2})',\n",
        "    ]\n",
        "    \n",
        "    start_time = None\n",
        "    end_time = None\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, time_input)\n",
        "        if match:\n",
        "            groups = match.groups()\n",
        "            \n",
        "            # Parse first time\n",
        "            if len(groups) >= 3:\n",
        "                hour1 = int(groups[0])\n",
        "                minute1 = int(groups[1]) if groups[1] else 0\n",
        "                ampm1 = groups[2].lower() if groups[2] else None\n",
        "                \n",
        "                if ampm1 == 'pm' and hour1 != 12:\n",
        "                    hour1 += 12\n",
        "                elif ampm1 == 'am' and hour1 == 12:\n",
        "                    hour1 = 0\n",
        "                \n",
        "                start_time = (hour1, minute1)\n",
        "                \n",
        "                # Parse second time\n",
        "                if len(groups) >= 6:\n",
        "                    hour2 = int(groups[3])\n",
        "                    minute2 = int(groups[4]) if groups[4] else 0\n",
        "                    ampm2 = groups[5].lower() if groups[5] else None\n",
        "                    \n",
        "                    if ampm2 == 'pm' and hour2 != 12:\n",
        "                        hour2 += 12\n",
        "                    elif ampm2 == 'am' and hour2 == 12:\n",
        "                        hour2 = 0\n",
        "                    \n",
        "                    end_time = (hour2, minute2)\n",
        "            \n",
        "            break\n",
        "    \n",
        "    if start_time is None or end_time is None:\n",
        "        raise ValueError(f\"Could not parse time range: {time_input}\")\n",
        "    \n",
        "    start_dt = base_date.replace(hour=start_time[0], minute=start_time[1], second=0, microsecond=0)\n",
        "    end_dt = base_date.replace(hour=end_time[0], minute=end_time[1], second=59, microsecond=999999)\n",
        "    \n",
        "    return start_dt, end_dt\n",
        "\n",
        "# TIME RANGE INPUT - Modify this value\n",
        "TIME_RANGE_INPUT = \"\"  # Empty for full day, or \"2pm to 4pm\", \"14:00 to 16:00\", etc.\n",
        "\n",
        "start_time_est, end_time_est = parse_time_range(TIME_RANGE_INPUT, target_date_est)\n",
        "\n",
        "print(f\"‚è∞ Time Range (EST): {start_time_est.strftime('%I:%M %p')} to {end_time_est.strftime('%I:%M %p')}\")\n",
        "print(f\"   UTC equivalent: {start_time_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S')} to {end_time_est.astimezone(UTC).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Convert to UTC for database queries\n",
        "start_time_utc = start_time_est.astimezone(UTC)\n",
        "end_time_utc = end_time_est.astimezone(UTC)\n",
        "\n",
        "print(f\"\\nüìä Querying database for activity between:\")\n",
        "print(f\"   EST: {start_time_est.strftime('%Y-%m-%d %I:%M %p')} - {end_time_est.strftime('%I:%M %p')}\")\n",
        "print(f\"   UTC: {start_time_utc.strftime('%Y-%m-%d %H:%M:%S')} - {end_time_utc.strftime('%H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Database Table Check\n",
        "\n",
        "Before fetching data, let's verify the required tables exist in your Supabase database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking database tables...\n",
            "\n",
            "‚úÖ Table 'activity_logs' exists\n",
            "‚ö†Ô∏è  Table 'users' exists but query failed: {'message': 'column users.id does not exist', 'code': '42703', 'hint': None, 'details': None}\n",
            "\n",
            "‚úÖ All required tables exist! Proceeding with data fetch...\n",
            "   Note: activity_logs will be filtered by ='tweet' to show only RephraseBot data\n"
          ]
        }
      ],
      "source": [
        "# Check if required tables exist\n",
        "print(\"üîç Checking database tables...\\n\")\n",
        "\n",
        "required_tables = [\"activity_logs\", \"users\"]\n",
        "existing_tables = []\n",
        "missing_tables = []\n",
        "\n",
        "for table_name in required_tables:\n",
        "    try:\n",
        "        # Try a simple query to check if table exists\n",
        "        result = supabase.table(table_name).select(\"id\").limit(1).execute()\n",
        "        existing_tables.append(table_name)\n",
        "        print(f\"‚úÖ Table '{table_name}' exists\")\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"PGRST205\" in error_msg or \"not found\" in error_msg.lower():\n",
        "            missing_tables.append(table_name)\n",
        "            print(f\"‚ùå Table '{table_name}' NOT FOUND\")\n",
        "        else:\n",
        "            # Table exists but query failed for another reason\n",
        "            existing_tables.append(table_name)\n",
        "            print(f\"‚ö†Ô∏è  Table '{table_name}' exists but query failed: {error_msg[:100]}\")\n",
        "\n",
        "if missing_tables:\n",
        "    print(f\"\\n‚ö†Ô∏è  MISSING TABLES: {', '.join(missing_tables)}\")\n",
        "    print(\"\\nüìù Please ensure these tables exist in your Supabase database.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All required tables exist! Proceeding with data fetch...\")\n",
        "    print(\"   Note: activity_logs will be filtered by ='tweet' to show only RephraseBot data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Fetching Data from Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching activity logs...\n",
            "‚úÖ Found 754 activity log entries (='tweet')\n",
            "\n",
            "Fetching user data...\n",
            "‚úÖ Found 200 total users in database\n",
            "\n",
            "üìà Data Summary:\n",
            "   Total log entries: 754\n",
            "   Unique users: 70\n",
            "   Date range: 2026-01-25 00:01:25.029712-05:00 to 2026-01-25 23:38:09.554803-05:00\n"
          ]
        }
      ],
      "source": [
        "# Fetch activity logs\n",
        "print(\"Fetching activity logs...\")\n",
        "# Note: activity_logs table (separate table for RephraseBot)\n",
        "try:\n",
        "    activity_logs = supabase.table(\"activity_logs\")\\\n",
        "        .select(\"*\")\\\n",
        "        .gte(\"timestamp\", start_time_utc.isoformat())\\\n",
        "        .lte(\"timestamp\", end_time_utc.isoformat())\\\n",
        "        .execute()\n",
        "    print(f\"‚úÖ Found {len(activity_logs.data)} activity log entries (='tweet')\")\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"activity_logs\" in error_msg and \"email_activity_logs\" in error_msg.lower():\n",
        "        print(\"‚ö†Ô∏è ERROR: Table 'activity_logs' not found in database.\")\n",
        "        print(\"   The table might not exist yet or might be named differently.\")\n",
        "        print(\"   Please ensure the table exists in Supabase with the name 'activity_logs'\")\n",
        "        print(\"   Or update the table name in this notebook if using a different name.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Fetch all users (for new user detection)\n",
        "print(\"\\nFetching user data...\")\n",
        "try:\n",
        "    all_users = supabase.table(\"users\")\\\n",
        "        .select(\"*\")\\\n",
        "        .execute()\n",
        "    print(f\"‚úÖ Found {len(all_users.data)} total users in database\")\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"users\" in error_msg:\n",
        "        print(\"‚ö†Ô∏è ERROR: Table 'users' not found in database.\")\n",
        "        print(\"   The table might not exist yet or might be named differently.\")\n",
        "        print(\"   Please ensure the table exists in Supabase with the name 'users'\")\n",
        "        print(\"   Or update the table name in this notebook if using a different name.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_logs = pd.DataFrame(activity_logs.data)\n",
        "df_users = pd.DataFrame(all_users.data)\n",
        "\n",
        "if len(df_logs) > 0:\n",
        "    # Convert timestamp to datetime and then to EST\n",
        "    # Parse timestamp - handle both timezone-aware and timezone-naive\n",
        "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])\n",
        "    \n",
        "    # Ensure timestamp is UTC-aware\n",
        "    if df_logs['timestamp'].dt.tz is None:\n",
        "        # Timezone-naive, localize to UTC\n",
        "        df_logs['timestamp_utc'] = df_logs['timestamp'].dt.tz_localize('UTC')\n",
        "    else:\n",
        "        # Already timezone-aware, convert to UTC\n",
        "        df_logs['timestamp_utc'] = df_logs['timestamp'].dt.tz_convert('UTC')\n",
        "    \n",
        "    # Convert to EST\n",
        "    df_logs['timestamp_est'] = df_logs['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
        "    df_logs['hour_est'] = df_logs['timestamp_est'].dt.hour\n",
        "    df_logs['date_est'] = df_logs['timestamp_est'].dt.date\n",
        "    \n",
        "    print(f\"\\nüìà Data Summary:\")\n",
        "    print(f\"   Total log entries: {len(df_logs)}\")\n",
        "    print(f\"   Unique users: {df_logs['user_id'].nunique()}\")\n",
        "    print(f\"   Date range: {df_logs['timestamp_est'].min()} to {df_logs['timestamp_est'].max()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No activity found for the specified time range\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• User Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üë• USER STATISTICS\n",
            "======================================================================\n",
            "\n",
            "üìä Overview:\n",
            "   Total unique users: 70\n",
            "   New users (first time): 35\n",
            "   Returning users: 35\n",
            "\n",
            "üÜï NEW USERS (35):\n",
            "   ‚Ä¢ Shadi (ID: 40970373) - 24 requests\n",
            "   ‚Ä¢ Mojde (ID: 174493019) - 15 requests\n",
            "   ‚Ä¢ User 72174318 (ID: 72174318) - 7 requests\n",
            "   ‚Ä¢ User 5779961681 (ID: 5779961681) - 7 requests\n",
            "   ‚Ä¢ Melika Shahbazi (ID: 720396774) - 6 requests\n",
            "   ‚Ä¢ ÿ≥⁄Ä€åÿØŸá ŸÑÿπŸÑ (ID: 373273536) - 4 requests\n",
            "   ‚Ä¢ Amir (ID: 91453135) - 4 requests\n",
            "   ‚Ä¢ Baharüå∏ (ID: 327313092) - 4 requests\n",
            "   ‚Ä¢ Hadis (ID: 789894653) - 4 requests\n",
            "   ‚Ä¢ User 482724810 (ID: 482724810) - 3 requests\n",
            "   ‚Ä¢ Arman Khoshnevis (ID: 110178849) - 2 requests\n",
            "   ‚Ä¢ Me (ID: 97821550) - 2 requests\n",
            "   ‚Ä¢ üëßüèª (ID: 84424830) - 2 requests\n",
            "   ‚Ä¢ User 100997008 (ID: 100997008) - 2 requests\n",
            "   ‚Ä¢ Farhad Daneshgar (ID: 72638551) - 1 requests\n",
            "   ‚Ä¢ User 99078447 (ID: 99078447) - 1 requests\n",
            "   ‚Ä¢ Marzie (ID: 81539195) - 1 requests\n",
            "   ‚Ä¢ User 84002905 (ID: 84002905) - 1 requests\n",
            "   ‚Ä¢ Navid (ID: 75383301) - 1 requests\n",
            "   ‚Ä¢ User 170419118 (ID: 170419118) - 1 requests\n",
            "   ‚Ä¢ Farri (ID: 166657724) - 1 requests\n",
            "   ‚Ä¢ Moein Naderi (ID: 126190704) - 1 requests\n",
            "   ‚Ä¢ User 117315824 (ID: 117315824) - 1 requests\n",
            "   ‚Ä¢ Nazanin (ID: 104481688) - 1 requests\n",
            "   ‚Ä¢ Leila Moradi (ID: 102800511) - 1 requests\n",
            "   ‚Ä¢ User 91639592 (ID: 91639592) - 1 requests\n",
            "   ‚Ä¢ User 239968890 (ID: 239968890) - 1 requests\n",
            "   ‚Ä¢ User 171354210 (ID: 171354210) - 1 requests\n",
            "   ‚Ä¢ Vida Araban (ID: 826876581) - 1 requests\n",
            "   ‚Ä¢ User 1567539598 (ID: 1567539598) - 1 requests\n",
            "   ‚Ä¢ User 1109951559 (ID: 1109951559) - 1 requests\n",
            "   ‚Ä¢ User 1782348431 (ID: 1782348431) - 1 requests\n",
            "   ‚Ä¢ Shahla Bagheri (ID: 5604085122) - 1 requests\n",
            "   ‚Ä¢ User 6272086386 (ID: 6272086386) - 1 requests\n",
            "   ‚Ä¢ User 8453189806 (ID: 8453189806) - 1 requests\n",
            "\n",
            "üë§ ALL ACTIVE USERS (70):\n",
            "   üîÑ RETURNING üÜì FREE User 5407949933 (ID: 5407949933) - 57 requests\n",
            "   üîÑ RETURNING üÜì FREE Alideeza (ID: 7430730600) - 42 requests\n",
            "   üîÑ RETURNING üÜì FREE Fariba Mohammadi (ID: 6509439762) - 38 requests\n",
            "   üîÑ RETURNING üÜì FREE Mazyar Z (ID: 72763667) - 37 requests\n",
            "   üîÑ RETURNING üÜì FREE Mhmd (ID: 123646820) - 35 requests\n",
            "   üîÑ RETURNING üÜì FREE Arian Madani (ID: 102669421) - 34 requests\n",
            "   üîÑ RETURNING üÜì FREE Homa Taheri (ID: 99421887) - 33 requests\n",
            "   üîÑ RETURNING üÜì FREE Shapour Mahaei (ID: 49041867) - 30 requests\n",
            "   üÜï NEW üÜì FREE Shadi (ID: 40970373) - 24 requests\n",
            "   üîÑ RETURNING üÜì FREE Maysam B13 (ID: 153609579) - 22 requests\n",
            "   üîÑ RETURNING üÜì FREE Amir Kucharski (ID: 657362230) - 22 requests\n",
            "   üîÑ RETURNING üíé PRO Niloufar Mohammadi (ID: 113064246) - 22 requests\n",
            "   üîÑ RETURNING üÜì FREE User 136051869 (ID: 136051869) - 21 requests\n",
            "   üîÑ RETURNING üÜì FREE User 100880089 (ID: 100880089) - 19 requests\n",
            "   üîÑ RETURNING üÜì FREE User 5789305318 (ID: 5789305318) - 19 requests\n",
            "   üîÑ RETURNING üÜì FREE Shiva (ID: 106912088) - 18 requests\n",
            "   üîÑ RETURNING üÜì FREE Siamak (ID: 108282521) - 18 requests\n",
            "   üîÑ RETURNING üÜì FREE Ysmn (ID: 403000069) - 17 requests\n",
            "   üîÑ RETURNING üÜì FREE User 442148297 (ID: 442148297) - 17 requests\n",
            "   üîÑ RETURNING üÜì FREE 2022 Spring (ID: 81206813) - 16 requests\n",
            "   üîÑ RETURNING üÜì FREE Hoda Irancel (ID: 106628373) - 16 requests\n",
            "   üÜï NEW üÜì FREE Mojde (ID: 174493019) - 15 requests\n",
            "   üîÑ RETURNING üíé PRO Shiva (ID: 7757464235) - 15 requests\n",
            "   üîÑ RETURNING üÜì FREE User 375082495 (ID: 375082495) - 15 requests\n",
            "   üîÑ RETURNING üÜì FREE User 97928586 (ID: 97928586) - 15 requests\n",
            "   üîÑ RETURNING üÜì FREE Mina(êé∑êé¥êé†) (ID: 479290855) - 11 requests\n",
            "   üîÑ RETURNING üÜì FREE alireza (ID: 111707568) - 11 requests\n",
            "   üîÑ RETURNING üÜì FREE User 210928631 (ID: 210928631) - 11 requests\n",
            "   üîÑ RETURNING üÜì FREE Sahar Ebrahimi Bajgani (ID: 1585075623) - 11 requests\n",
            "   üÜï NEW üÜì FREE User 72174318 (ID: 72174318) - 7 requests\n",
            "   üÜï NEW üÜì FREE User 5779961681 (ID: 5779961681) - 7 requests\n",
            "   üÜï NEW üÜì FREE Melika Shahbazi (ID: 720396774) - 6 requests\n",
            "   üîÑ RETURNING üÜì FREE Shaghayegh (ID: 301297212) - 6 requests\n",
            "   üîÑ RETURNING üÜì FREE Arezoo (ID: 103719723) - 5 requests\n",
            "   üÜï NEW üÜì FREE Hadis (ID: 789894653) - 4 requests\n",
            "   üÜï NEW üÜì FREE ÿ≥⁄Ä€åÿØŸá ŸÑÿπŸÑ (ID: 373273536) - 4 requests\n",
            "   üÜï NEW üÜì FREE Amir (ID: 91453135) - 4 requests\n",
            "   üÜï NEW üÜì FREE Baharüå∏ (ID: 327313092) - 4 requests\n",
            "   üîÑ RETURNING üÜì FREE Amin (ID: 422339974) - 3 requests\n",
            "   üîÑ RETURNING üÜì FREE Soha (ID: 135524395) - 3 requests\n",
            "   üîÑ RETURNING üÜì FREE User 109496653 (ID: 109496653) - 3 requests\n",
            "   üÜï NEW üÜì FREE User 482724810 (ID: 482724810) - 3 requests\n",
            "   üÜï NEW üÜì FREE User 100997008 (ID: 100997008) - 2 requests\n",
            "   üÜï NEW üÜì FREE Arman Khoshnevis (ID: 110178849) - 2 requests\n",
            "   üîÑ RETURNING üÜì FREE User 1133010573 (ID: 1133010573) - 2 requests\n",
            "   üîÑ RETURNING üÜì FREE Khojasteh Nz (ID: 474388934) - 2 requests\n",
            "   üÜï NEW üÜì FREE üëßüèª (ID: 84424830) - 2 requests\n",
            "   üÜï NEW üÜì FREE Me (ID: 97821550) - 2 requests\n",
            "   üÜï NEW üÜì FREE Navid (ID: 75383301) - 1 requests\n",
            "   üÜï NEW üÜì FREE Farhad Daneshgar (ID: 72638551) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 91639592 (ID: 91639592) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 84002905 (ID: 84002905) - 1 requests\n",
            "   üÜï NEW üÜì FREE Marzie (ID: 81539195) - 1 requests\n",
            "   üÜï NEW üÜì FREE Nazanin (ID: 104481688) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 171354210 (ID: 171354210) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 170419118 (ID: 170419118) - 1 requests\n",
            "   üÜï NEW üÜì FREE Farri (ID: 166657724) - 1 requests\n",
            "   üÜï NEW üÜì FREE Moein Naderi (ID: 126190704) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 117315824 (ID: 117315824) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 99078447 (ID: 99078447) - 1 requests\n",
            "   üÜï NEW üÜì FREE Leila Moradi (ID: 102800511) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 239968890 (ID: 239968890) - 1 requests\n",
            "   üÜï NEW üÜì FREE Shahla Bagheri (ID: 5604085122) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 1782348431 (ID: 1782348431) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 1109951559 (ID: 1109951559) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 1567539598 (ID: 1567539598) - 1 requests\n",
            "   üÜï NEW üÜì FREE Vida Araban (ID: 826876581) - 1 requests\n",
            "   üîÑ RETURNING üÜì FREE User 5759174819 (ID: 5759174819) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 6272086386 (ID: 6272086386) - 1 requests\n",
            "   üÜï NEW üÜì FREE User 8453189806 (ID: 8453189806) - 1 requests\n",
            "\n",
            "üèÜ TOP 10 USERS BY REQUESTS:\n",
            "    57 requests - User 5407949933 (ID: 5407949933)\n",
            "    42 requests - Alideeza (ID: 7430730600)\n",
            "    38 requests - Fariba Mohammadi (ID: 6509439762)\n",
            "    37 requests - Mazyar Z (ID: 72763667)\n",
            "    35 requests - Mhmd (ID: 123646820)\n",
            "    34 requests - Arian Madani (ID: 102669421)\n",
            "    33 requests - Homa Taheri (ID: 99421887)\n",
            "    30 requests - Shapour Mahaei (ID: 49041867)\n",
            "    24 requests - Shadi (ID: 40970373)\n",
            "    22 requests - Maysam B13 (ID: 153609579)\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    # Get all unique user IDs from the time period\n",
        "    active_user_ids = df_logs['user_id'].unique()\n",
        "    \n",
        "    # Find new users (first activity in this time period)\n",
        "    # Get first activity timestamp for each user in the period\n",
        "    first_activities = df_logs.groupby('user_id')['timestamp_utc'].min().reset_index()\n",
        "    first_activities.columns = ['user_id', 'first_activity_utc']\n",
        "    \n",
        "    # Check if this is their first activity ever (before this time period)\n",
        "    new_users = []\n",
        "    for user_id in active_user_ids:\n",
        "        user_first_activity = first_activities[first_activities['user_id'] == user_id]['first_activity_utc'].iloc[0]\n",
        "        \n",
        "        # Check if there's any activity before this time period\n",
        "        earlier_activity = supabase.table(\"activity_logs\")\\\n",
        "            .select(\"timestamp\")\\\n",
        "            .eq(\"user_id\", int(user_id))\\\n",
        "            .lt(\"timestamp\", start_time_utc.isoformat())\\\n",
        "            .limit(1)\\\n",
        "            .execute()\n",
        "        \n",
        "        if len(earlier_activity.data) == 0:\n",
        "            new_users.append(user_id)\n",
        "    \n",
        "    # Get user details\n",
        "    user_stats = df_logs.groupby('user_id').agg({\n",
        "        'action_type': 'count',\n",
        "        'timestamp_est': ['min', 'max'],\n",
        "        'response_time_ms': 'mean'\n",
        "    }).reset_index()\n",
        "    user_stats.columns = ['user_id', 'total_requests', 'first_request_est', 'last_request_est', 'avg_response_time_ms']\n",
        "    \n",
        "    # Merge with user info\n",
        "    if len(df_users) > 0:\n",
        "        user_stats = user_stats.merge(\n",
        "            df_users[['user_id', 'first_name', 'last_name', 'username', 'is_pro', 'pro_expires_at', 'trial_ends_at']],\n",
        "            on='user_id',\n",
        "            how='left'\n",
        "        )\n",
        "        user_stats['display_name'] = user_stats.apply(\n",
        "            lambda x: f\"{x['first_name'] or ''} {x['last_name'] or ''}\".strip() or f\"@{x['username']}\" if x['username'] else f\"User {x['user_id']}\",\n",
        "            axis=1\n",
        "        )\n",
        "    else:\n",
        "        user_stats['display_name'] = user_stats['user_id'].apply(lambda x: f\"User {x}\")\n",
        "    \n",
        "    user_stats['is_new_user'] = user_stats['user_id'].isin(new_users)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë• USER STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüìä Overview:\")\n",
        "    print(f\"   Total unique users: {len(active_user_ids)}\")\n",
        "    print(f\"   New users (first time): {len(new_users)}\")\n",
        "    print(f\"   Returning users: {len(active_user_ids) - len(new_users)}\")\n",
        "    \n",
        "    # New users list\n",
        "    if len(new_users) > 0:\n",
        "        print(f\"\\nüÜï NEW USERS ({len(new_users)}):\")\n",
        "        new_user_df = user_stats[user_stats['is_new_user']].sort_values('total_requests', ascending=False)\n",
        "        for idx, row in new_user_df.iterrows():\n",
        "            print(f\"   ‚Ä¢ {row['display_name']} (ID: {int(row['user_id'])}) - {int(row['total_requests'])} requests\")\n",
        "    else:\n",
        "        print(f\"\\nüÜï NEW USERS: None\")\n",
        "    \n",
        "    # All active users\n",
        "    print(f\"\\nüë§ ALL ACTIVE USERS ({len(user_stats)}):\")\n",
        "    user_stats_sorted = user_stats.sort_values('total_requests', ascending=False)\n",
        "    for idx, row in user_stats_sorted.iterrows():\n",
        "        status = \"üÜï NEW\" if row['is_new_user'] else \"üîÑ RETURNING\"\n",
        "        pro_status = \"üíé PRO\" if row.get('is_pro') else \"üÜì FREE\"\n",
        "        print(f\"   {status} {pro_status} {row['display_name']} (ID: {int(row['user_id'])}) - {int(row['total_requests'])} requests\")\n",
        "    \n",
        "    # Top users by requests\n",
        "    print(f\"\\nüèÜ TOP 10 USERS BY REQUESTS:\")\n",
        "    top_users = user_stats_sorted.head(10)\n",
        "    for idx, row in top_users.iterrows():\n",
        "        print(f\"   {int(row['total_requests']):3d} requests - {row['display_name']} (ID: {int(row['user_id'])})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Request Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üìä REQUEST STATISTICS\n",
            "======================================================================\n",
            "\n",
            "üìà Total Requests: 754\n",
            "\n",
            "üìã Requests by Action Type:\n",
            "   rephrase_success         :  602 ( 79.8%)\n",
            "   rate_limited             :   90 ( 11.9%)\n",
            "   command_start            :   47 (  6.2%)\n",
            "   persian_error            :   12 (  1.6%)\n",
            "   invalid_channel          :    3 (  0.4%)\n",
            "\n",
            "‚úÖ Success Rate:\n",
            "   Successful rephrases: 602 (79.8%)\n",
            "   Errors: 105 (13.9%)\n",
            "   Other actions: 47 (6.2%)\n",
            "\n",
            "‚è±Ô∏è  Response Time Statistics (for successful requests):\n",
            "   Average: 2425 ms (2.43 seconds)\n",
            "   Median:  2058 ms (2.06 seconds)\n",
            "   Min:     1678 ms (1.68 seconds)\n",
            "   Max:     8677 ms (8.68 seconds)\n",
            "   P95:     4181 ms (4.18 seconds)\n",
            "\n",
            "üìè Message Length Statistics:\n",
            "   Average original length: 198 characters\n",
            "   Median original length:  189 characters\n",
            "   Min length: 74 characters\n",
            "   Max length: 1424 characters\n",
            "   Average rephrased length: 181 characters\n",
            "   Median rephrased length:  180 characters\n",
            "   Average length change: -15 characters\n",
            "\n",
            "üéØ Similarity Score Statistics:\n",
            "   Average similarity: 0.278\n",
            "   Median similarity:  0.278\n",
            "   Min similarity: 0.000\n",
            "   Max similarity: 0.527\n",
            "\n",
            "üîó X/Twitter Link Tracking:\n",
            "   Rephrases with X links: 524 (87.0%)\n",
            "\n",
            "üì§ Forwarded Messages:\n",
            "   Rephrases from forwarded messages: 602 (100.0%)\n",
            "\n",
            "‚ùå Error Breakdown:\n",
            "   rate_limited             :   90\n",
            "   persian_error            :   12\n",
            "   invalid_channel          :    3\n",
            "\n",
            "   Most common error messages:\n",
            "      ‚Ä¢ Wait 1 seconds: 17\n",
            "      ‚Ä¢ Wait 2 seconds: 11\n",
            "      ‚Ä¢ Wait 5 seconds: 10\n",
            "      ‚Ä¢ Wait 3 seconds: 9\n",
            "      ‚Ä¢ Wait 7 seconds: 8\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä REQUEST STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Total requests\n",
        "    total_requests = len(df_logs)\n",
        "    print(f\"\\nüìà Total Requests: {total_requests}\")\n",
        "    \n",
        "    # Requests by action type\n",
        "    print(f\"\\nüìã Requests by Action Type:\")\n",
        "    action_counts = df_logs['action_type'].value_counts()\n",
        "    for action, count in action_counts.items():\n",
        "        percentage = (count / total_requests) * 100\n",
        "        print(f\"   {action:25s}: {count:4d} ({percentage:5.1f}%)\")\n",
        "    \n",
        "    # Success vs Error\n",
        "    success_count = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "    error_count = len(df_logs[df_logs['action_type'].isin([\n",
        "        'rephrase_failed', \n",
        "        'rate_limited', \n",
        "        'telegram_api_error', \n",
        "        'bot_blocked_by_user',\n",
        "        'forwarded_media_error',\n",
        "        'multiple_links_error',\n",
        "        'persian_error',\n",
        "        'invalid_channel'\n",
        "    ])])\n",
        "    other_count = total_requests - success_count - error_count\n",
        "    \n",
        "    print(f\"\\n‚úÖ Success Rate:\")\n",
        "    print(f\"   Successful rephrases: {success_count} ({(success_count/total_requests*100):.1f}%)\")\n",
        "    print(f\"   Errors: {error_count} ({(error_count/total_requests*100):.1f}%)\")\n",
        "    print(f\"   Other actions: {other_count} ({(other_count/total_requests*100):.1f}%)\")\n",
        "    \n",
        "    # Response time statistics\n",
        "    successful_logs = df_logs[df_logs['action_type'] == 'rephrase_success']\n",
        "    if len(successful_logs) > 0 and 'response_time_ms' in successful_logs.columns:\n",
        "        response_times = successful_logs['response_time_ms'].dropna()\n",
        "        if len(response_times) > 0:\n",
        "            print(f\"\\n‚è±Ô∏è  Response Time Statistics (for successful requests):\")\n",
        "            print(f\"   Average: {response_times.mean():.0f} ms ({response_times.mean()/1000:.2f} seconds)\")\n",
        "            print(f\"   Median:  {response_times.median():.0f} ms ({response_times.median()/1000:.2f} seconds)\")\n",
        "            print(f\"   Min:     {response_times.min():.0f} ms ({response_times.min()/1000:.2f} seconds)\")\n",
        "            print(f\"   Max:     {response_times.max():.0f} ms ({response_times.max()/1000:.2f} seconds)\")\n",
        "            print(f\"   P95:     {response_times.quantile(0.95):.0f} ms ({response_times.quantile(0.95)/1000:.2f} seconds)\")\n",
        "    \n",
        "    # Message/Tweet length statistics\n",
        "    if 'original_length' in df_logs.columns:\n",
        "        original_lengths = df_logs['original_length'].dropna()\n",
        "        if len(original_lengths) > 0:\n",
        "            print(f\"\\nüìè Message Length Statistics:\")\n",
        "            print(f\"   Average original length: {original_lengths.mean():.0f} characters\")\n",
        "            print(f\"   Median original length:  {original_lengths.median():.0f} characters\")\n",
        "            print(f\"   Min length: {original_lengths.min():.0f} characters\")\n",
        "            print(f\"   Max length: {original_lengths.max():.0f} characters\")\n",
        "    \n",
        "    if 'rephrased_length' in df_logs.columns:\n",
        "        rephrased_lengths = df_logs[df_logs['action_type'] == 'rephrase_success']['rephrased_length'].dropna()\n",
        "        if len(rephrased_lengths) > 0:\n",
        "            print(f\"   Average rephrased length: {rephrased_lengths.mean():.0f} characters\")\n",
        "            print(f\"   Median rephrased length:  {rephrased_lengths.median():.0f} characters\")\n",
        "            \n",
        "            # Length change\n",
        "            successful_with_both = df_logs[\n",
        "                (df_logs['action_type'] == 'rephrase_success') & \n",
        "                df_logs['original_length'].notna() & \n",
        "                df_logs['rephrased_length'].notna()\n",
        "            ]\n",
        "            if len(successful_with_both) > 0:\n",
        "                length_changes = successful_with_both['rephrased_length'] - successful_with_both['original_length']\n",
        "                print(f\"   Average length change: {length_changes.mean():+.0f} characters\")\n",
        "    \n",
        "    # Similarity score statistics\n",
        "    if 'similarity_score' in df_logs.columns:\n",
        "        similarity_scores = df_logs[df_logs['action_type'] == 'rephrase_success']['similarity_score'].dropna()\n",
        "        if len(similarity_scores) > 0:\n",
        "            print(f\"\\nüéØ Similarity Score Statistics:\")\n",
        "            print(f\"   Average similarity: {similarity_scores.mean():.3f}\")\n",
        "            print(f\"   Median similarity:  {similarity_scores.median():.3f}\")\n",
        "            print(f\"   Min similarity: {similarity_scores.min():.3f}\")\n",
        "            print(f\"   Max similarity: {similarity_scores.max():.3f}\")\n",
        "    \n",
        "    # X/Twitter link tracking\n",
        "    if 'had_x_link' in df_logs.columns:\n",
        "        x_link_count = df_logs[df_logs['action_type'] == 'rephrase_success']['had_x_link'].sum()\n",
        "        total_success = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "        if total_success > 0:\n",
        "            print(f\"\\nüîó X/Twitter Link Tracking:\")\n",
        "            print(f\"   Rephrases with X links: {int(x_link_count)} ({x_link_count/total_success*100:.1f}%)\")\n",
        "    \n",
        "    # Forwarded messages tracking\n",
        "    if 'was_forwarded' in df_logs.columns:\n",
        "        forwarded_count = df_logs[df_logs['action_type'] == 'rephrase_success']['was_forwarded'].sum()\n",
        "        total_success = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "        if total_success > 0:\n",
        "            print(f\"\\nüì§ Forwarded Messages:\")\n",
        "            print(f\"   Rephrases from forwarded messages: {int(forwarded_count)} ({forwarded_count/total_success*100:.1f}%)\")\n",
        "    \n",
        "    # Error breakdown\n",
        "    error_logs = df_logs[df_logs['action_type'].isin([\n",
        "        'rephrase_failed', \n",
        "        'rate_limited', \n",
        "        'telegram_api_error', \n",
        "        'bot_blocked_by_user',\n",
        "        'forwarded_media_error',\n",
        "        'multiple_links_error',\n",
        "        'persian_error',\n",
        "        'invalid_channel'\n",
        "    ])]\n",
        "    if len(error_logs) > 0:\n",
        "        print(f\"\\n‚ùå Error Breakdown:\")\n",
        "        error_types = error_logs['action_type'].value_counts()\n",
        "        for error_type, count in error_types.items():\n",
        "            print(f\"   {error_type:25s}: {count:4d}\")\n",
        "        \n",
        "        # Error messages (if available)\n",
        "        if 'error_message' in error_logs.columns:\n",
        "            error_messages = error_logs[error_logs['error_message'].notna()]['error_message'].value_counts()\n",
        "            if len(error_messages) > 0:\n",
        "                print(f\"\\n   Most common error messages:\")\n",
        "                for msg, count in error_messages.head(5).items():\n",
        "                    print(f\"      ‚Ä¢ {msg[:60]}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚è∞ Activity Timeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "‚è∞ ACTIVITY TIMELINE (EST)\n",
            "======================================================================\n",
            "\n",
            "üìä Activity by Hour (EST):\n",
            "    0:00 EST:   73 requests ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    1:00 EST:    4 requests \n",
            "    2:00 EST:    6 requests \n",
            "    3:00 EST:    1 requests \n",
            "    4:00 EST:    2 requests \n",
            "    8:00 EST:   16 requests ‚ñà\n",
            "    9:00 EST:   15 requests ‚ñà\n",
            "   10:00 EST:    5 requests \n",
            "   12:00 EST:    2 requests \n",
            "   13:00 EST:    6 requests \n",
            "   14:00 EST:    1 requests \n",
            "   15:00 EST:    1 requests \n",
            "   17:00 EST:    1 requests \n",
            "   19:00 EST:    6 requests \n",
            "   20:00 EST:    3 requests \n",
            "   21:00 EST:  523 requests ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "   22:00 EST:   51 requests ‚ñà‚ñà‚ñà‚ñà\n",
            "   23:00 EST:   38 requests ‚ñà‚ñà‚ñà\n",
            "\n",
            "   üèÜ Peak hour: 21:00 EST with 523 requests\n",
            "\n",
            "üìÖ Activity Timeline:\n",
            "   First activity: 2026-01-25 12:01:25 AM EST\n",
            "   Last activity:  2026-01-25 11:38:09 PM EST\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚è∞ ACTIVITY TIMELINE (EST)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Activity by hour\n",
        "    print(f\"\\nüìä Activity by Hour (EST):\")\n",
        "    hourly_activity = df_logs.groupby('hour_est').size().reset_index(name='count')\n",
        "    hourly_activity = hourly_activity.sort_values('hour_est')\n",
        "    \n",
        "    max_count = hourly_activity['count'].max()\n",
        "    \n",
        "    for _, row in hourly_activity.iterrows():\n",
        "        hour = int(row['hour_est'])\n",
        "        count = int(row['count'])\n",
        "        hour_str = f\"{hour:2d}:00\"\n",
        "        bar_length = int((count / max_count) * 50) if max_count > 0 else 0\n",
        "        bar = \"‚ñà\" * bar_length\n",
        "        print(f\"   {hour_str} EST: {count:4d} requests {bar}\")\n",
        "    \n",
        "    # Peak hours\n",
        "    peak_hour = hourly_activity.loc[hourly_activity['count'].idxmax()]\n",
        "    print(f\"\\n   üèÜ Peak hour: {int(peak_hour['hour_est']):02d}:00 EST with {int(peak_hour['count'])} requests\")\n",
        "    \n",
        "    # Activity timeline (first and last activity)\n",
        "    print(f\"\\nüìÖ Activity Timeline:\")\n",
        "    print(f\"   First activity: {df_logs['timestamp_est'].min().strftime('%Y-%m-%d %I:%M:%S %p %Z')}\")\n",
        "    print(f\"   Last activity:  {df_logs['timestamp_est'].max().strftime('%Y-%m-%d %I:%M:%S %p %Z')}\")\n",
        "    \n",
        "    # Requests per minute (if time range is small)\n",
        "    time_span = (end_time_est - start_time_est).total_seconds() / 60  # minutes\n",
        "    if time_span <= 60:  # If less than 1 hour, show per-minute breakdown\n",
        "        print(f\"\\nüìä Activity by Minute (EST):\")\n",
        "        df_logs['minute_est'] = df_logs['timestamp_est'].dt.floor('T')\n",
        "        minute_activity = df_logs.groupby('minute_est').size().reset_index(name='count')\n",
        "        minute_activity = minute_activity.sort_values('minute_est')\n",
        "        \n",
        "        for _, row in minute_activity.iterrows():\n",
        "            minute = row['minute_est']\n",
        "            count = int(row['count'])\n",
        "            print(f\"   {minute.strftime('%H:%M')} EST: {count} requests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíé Pro vs Free Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üíé PRO vs FREE USERS\n",
            "======================================================================\n",
            "\n",
            "üë• User Count:\n",
            "   Pro users:  2\n",
            "   Free users: 68\n",
            "\n",
            "üìä Request Count:\n",
            "   Pro requests:  37 (4.9%)\n",
            "   Free requests: 717 (95.1%)\n",
            "   Average requests per Pro user: 18.5\n",
            "   Average requests per Free user: 10.5\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üíé PRO vs FREE USERS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Merge with user data to get Pro status\n",
        "    if len(df_users) > 0:\n",
        "        df_logs_with_users = df_logs.merge(\n",
        "            df_users[['user_id', 'is_pro', 'pro_expires_at', 'trial_ends_at']],\n",
        "            on='user_id',\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Determine Pro status (check expiration dates)\n",
        "        now_utc = datetime.now(UTC)\n",
        "        \n",
        "        def is_pro_active(row):\n",
        "            if pd.isna(row.get('is_pro')) or not row.get('is_pro'):\n",
        "                # Check trial\n",
        "                if pd.notna(row.get('trial_ends_at')):\n",
        "                    trial_end = pd.to_datetime(row['trial_ends_at'])\n",
        "                    if trial_end.tz is None:\n",
        "                        trial_end = UTC.localize(trial_end)\n",
        "                    return trial_end > now_utc\n",
        "                return False\n",
        "            \n",
        "            # Check Pro expiration\n",
        "            if pd.isna(row.get('pro_expires_at')):\n",
        "                return True  # Lifetime Pro\n",
        "            \n",
        "            pro_end = pd.to_datetime(row['pro_expires_at'])\n",
        "            if pro_end.tz is None:\n",
        "                pro_end = UTC.localize(pro_end)\n",
        "            return pro_end > now_utc\n",
        "        \n",
        "        df_logs_with_users['is_pro_active'] = df_logs_with_users.apply(is_pro_active, axis=1)\n",
        "        \n",
        "        # Pro vs Free statistics\n",
        "        pro_users = df_logs_with_users[df_logs_with_users['is_pro_active'] == True]['user_id'].nunique()\n",
        "        free_users = df_logs_with_users[df_logs_with_users['is_pro_active'] == False]['user_id'].nunique()\n",
        "        \n",
        "        pro_requests = len(df_logs_with_users[df_logs_with_users['is_pro_active'] == True])\n",
        "        free_requests = len(df_logs_with_users[df_logs_with_users['is_pro_active'] == False])\n",
        "        \n",
        "        print(f\"\\nüë• User Count:\")\n",
        "        print(f\"   Pro users:  {pro_users}\")\n",
        "        print(f\"   Free users: {free_users}\")\n",
        "        \n",
        "        print(f\"\\nüìä Request Count:\")\n",
        "        print(f\"   Pro requests:  {pro_requests} ({(pro_requests/len(df_logs)*100):.1f}%)\")\n",
        "        print(f\"   Free requests: {free_requests} ({(free_requests/len(df_logs)*100):.1f}%)\")\n",
        "        \n",
        "        if pro_users > 0:\n",
        "            avg_pro_requests = pro_requests / pro_users\n",
        "            print(f\"   Average requests per Pro user: {avg_pro_requests:.1f}\")\n",
        "        \n",
        "        if free_users > 0:\n",
        "            avg_free_requests = free_requests / free_users\n",
        "            print(f\"   Average requests per Free user: {avg_free_requests:.1f}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è User data not available for Pro/Free analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üìà SUMMARY REPORT\n",
            "======================================================================\n",
            "\n",
            "üìÖ Date: 2026-01-25 (Sunday)\n",
            "‚è∞ Time Range: 12:00 AM - 11:59 PM EST\n",
            "\n",
            "üìä Key Metrics:\n",
            "   ‚Ä¢ Total Requests: 754\n",
            "   ‚Ä¢ Unique Users: 70\n",
            "   ‚Ä¢ Successful Rephrases: 602 (79.8%)\n",
            "   ‚Ä¢ New Users: 35\n",
            "   ‚Ä¢ Avg Response Time: 2.43 seconds\n",
            "   ‚Ä¢ Peak Hour: 21:00 EST (523 requests)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "if len(df_logs) == 0:\n",
        "    print(\"No data to analyze.\")\n",
        "else:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìà SUMMARY REPORT\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüìÖ Date: {target_date_est.strftime('%Y-%m-%d (%A)')}\")\n",
        "    print(f\"‚è∞ Time Range: {start_time_est.strftime('%I:%M %p')} - {end_time_est.strftime('%I:%M %p')} EST\")\n",
        "    print(f\"\\nüìä Key Metrics:\")\n",
        "    print(f\"   ‚Ä¢ Total Requests: {len(df_logs)}\")\n",
        "    print(f\"   ‚Ä¢ Unique Users: {df_logs['user_id'].nunique()}\")\n",
        "    \n",
        "    success_count = len(df_logs[df_logs['action_type'] == 'rephrase_success'])\n",
        "    print(f\"   ‚Ä¢ Successful Rephrases: {success_count} ({(success_count/len(df_logs)*100):.1f}%)\")\n",
        "    \n",
        "    if len(new_users) > 0:\n",
        "        print(f\"   ‚Ä¢ New Users: {len(new_users)}\")\n",
        "    \n",
        "    if len(df_logs) > 0:\n",
        "        response_times = df_logs[df_logs['action_type'] == 'rephrase_success']['response_time_ms'].dropna()\n",
        "        if len(response_times) > 0:\n",
        "            print(f\"   ‚Ä¢ Avg Response Time: {response_times.mean()/1000:.2f} seconds\")\n",
        "    \n",
        "    peak_hour_data = df_logs.groupby('hour_est').size()\n",
        "    if len(peak_hour_data) > 0:\n",
        "        peak_hour = peak_hour_data.idxmax()\n",
        "        peak_count = peak_hour_data.max()\n",
        "        print(f\"   ‚Ä¢ Peak Hour: {int(peak_hour):02d}:00 EST ({peak_count} requests)\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
